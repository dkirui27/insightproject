{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import streamlit as st\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import os\n",
    "import altair as alt\n",
    "import pydeck as pdk\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib\n",
    "import sklearn\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import classification_report \n",
    "\n",
    "import lightgbm as lgb\n",
    "from fbprophet import Prophet\n",
    "from pmdarima import auto_arima\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#importing a scoring metric to compare methods\n",
    "from sklearn.metrics import r2_score\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(\"../data/processed/train_features.csv\")\n",
    "train_labels = pd.read_csv(\"../data/processed/train_labels.csv\")\n",
    "test_features = pd.read_csv(\"../data/processed/test_features.csv\")\n",
    "test_labels = pd.read_csv(\"../data/processed/test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.set_index(['measurement_date', 'user_code'], inplace=True) #reseting the index\n",
    "train_labels.set_index(['measurement_date', 'user_code'], inplace=True) #reseting the index\n",
    "test_features.set_index(['measurement_date', 'user_code'], inplace=True) #reseting the index\n",
    "test_labels.set_index(['measurement_date', 'user_code'], inplace=True) #reseting the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with 200 trees\n",
    "\n",
    "#setting random number seed\n",
    "myfavoritenumber = 13\n",
    "seed = myfavoritenumber\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=200, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt',\n",
    "                               class_weight = {0:5,1:1}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model.fit(train_features, train_labels.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_predictions = mod.predict(test_features)\n",
    "rf_probs = mod.predict_proba(test_features)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8783111149584488"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate roc auc\n",
    "roc_value = roc_auc_score(test_labels, rf_probs)\n",
    "roc_value #HRV still in there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "results = confusion_matrix(test_labels, rf_predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix :\n",
      "[[560  48]\n",
      " [ 62  52]]\n",
      "Accuracy Score : 0.8476454293628809\n",
      "Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.92      0.91       608\n",
      "           1       0.52      0.46      0.49       114\n",
      "\n",
      "    accuracy                           0.85       722\n",
      "   macro avg       0.71      0.69      0.70       722\n",
      "weighted avg       0.84      0.85      0.84       722\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Confusion Matrix :')\n",
    "print(results) \n",
    "print('Accuracy Score :',accuracy_score(test_labels, rf_predictions)) \n",
    "print('Report : ')\n",
    "print(classification_report(test_labels, rf_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
